{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb296c0d",
   "metadata": {},
   "source": [
    "# Training Data Preparation: Triple-Barrier (Bollinger Mean Reversion)\n",
    "\n",
    "This notebook prepares the final training dataset for the triple-barrier approach with Bollinger Band mean reversion strategy by:\n",
    "\n",
    "1. **Loading features** from `features_triple_barrier.csv`\n",
    "2. **Loading labels** from triple-barrier events CSV (generated by meta_labeling.ipynb)\n",
    "3. **Merging** features with labels on timestamp\n",
    "4. **Computing sample weights** using concurrency and return attribution\n",
    "5. **Preprocessing features** with MinMax normalization\n",
    "6. **Time-based train/test split** (no shuffling to preserve temporal order)\n",
    "7. **Saving ready-to-use datasets** for model training\n",
    "\n",
    "**Strategy Context:**\n",
    "- Primary model: Bollinger Band mean reversion (window=20, num_std=2.0)\n",
    "- Entry filter: CUSUM filter on volatility\n",
    "- Triple-barrier settings: pt_sl=[1, 2], vertical_barrier=50 bars\n",
    "- Can be changed to MA crossover later for comparison with trend-scanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d649a",
   "metadata": {},
   "source": [
    "## 1. Load Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe72a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "features_file = 'data/features_triple_barrier.csv'\n",
    "print(f\"Loading features from {features_file}...\")\n",
    "features = pd.read_csv(features_file, index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"✓ Features loaded: {features.shape}\")\n",
    "print(f\"  Date range: {features.index[0]} to {features.index[-1]}\")\n",
    "print(f\"  Columns: {len(features.columns)}\")\n",
    "features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cca239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load the triple-barrier events file\n",
    "data_dir = Path('data')\n",
    "label_files = list(data_dir.glob('EURUSD_triple_barrier_events_unfiltered.csv'))\n",
    "\n",
    "if not label_files:\n",
    "    raise FileNotFoundError(\"No triple-barrier events file found! Run meta_labeling.ipynb first.\")\n",
    "\n",
    "# Use the most recent file if multiple exist\n",
    "labels_file = sorted(label_files)[-1]\n",
    "print(f\"Loading labels from {labels_file.name}...\")\n",
    "labels = pd.read_csv(labels_file, index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"✓ Labels loaded: {labels.shape}\")\n",
    "print(f\"  Date range: {labels.index[0]} to {labels.index[-1]}\")\n",
    "print(f\"  Columns: {list(labels.columns)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "if 'bin' in labels.columns:\n",
    "    label_dist = labels['bin'].value_counts().sort_index()\n",
    "    for label, count in label_dist.items():\n",
    "        pct = count / len(labels) * 100\n",
    "        label_name = {-1: 'STOP_LOSS', 0: 'TIMEOUT', 1: 'PROFIT_TARGET'}.get(label, 'UNKNOWN')\n",
    "        print(f\"  {label_name:15s} ({label:2d}): {count:6,} ({pct:5.2f}%)\")\n",
    "labels.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5300d0",
   "metadata": {},
   "source": [
    "## 2. Merge Features with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on timestamp (inner join to keep only observations with both features and labels)\n",
    "print(\"Merging features with labels...\")\n",
    "print(f\"  Features: {len(features):,} rows\")\n",
    "print(f\"  Labels: {len(labels):,} rows\")\n",
    "\n",
    "# Inner join on index (timestamp)\n",
    "data = features.join(labels, how='inner', rsuffix='_label')\n",
    "\n",
    "print(f\"\\n✓ Merged dataset: {data.shape}\")\n",
    "print(f\"  Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "print(f\"  Lost {len(features) - len(data):,} observations due to missing labels\")\n",
    "\n",
    "# Verify no missing values in critical columns\n",
    "print(f\"\\nMissing values check:\")\n",
    "print(f\"  Features: {data[features.columns].isnull().sum().sum()}\")\n",
    "print(f\"  Labels (bin): {data['bin'].isnull().sum()}\")\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdefc01",
   "metadata": {},
   "source": [
    "## 3. Prepare Features, Labels, and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features, labels, and metadata\n",
    "feature_cols = features.columns.tolist()\n",
    "X = data[feature_cols].copy()\n",
    "y = data['bin'].copy()\n",
    "\n",
    "# Extract metadata columns (may vary depending on triple_barrier output)\n",
    "metadata_cols = [col for col in labels.columns if col != 'bin']\n",
    "label_metadata = data[metadata_cols].copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET COMPOSITION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Labels (y): {y.shape}\")\n",
    "print(f\"Label metadata: {label_metadata.shape}\")\n",
    "print(f\"  Metadata columns: {list(label_metadata.columns)}\")\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "label_counts = y.value_counts().sort_index()\n",
    "for label, count in label_counts.items():\n",
    "    pct = count / len(y) * 100\n",
    "    label_name = {-1: 'STOP_LOSS', 0: 'TIMEOUT', 1: 'PROFIT_TARGET'}.get(label, 'UNKNOWN')\n",
    "    print(f\"  {label_name:15s} ({label:2d}): {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Check if return information is available\n",
    "if 'ret' in label_metadata.columns:\n",
    "    print(f\"\\nReturn statistics:\")\n",
    "    print(f\"  Mean: {label_metadata['ret'].mean():.6f}\")\n",
    "    print(f\"  Median: {label_metadata['ret'].median():.6f}\")\n",
    "    print(f\"  Std: {label_metadata['ret'].std():.6f}\")\n",
    "    print(f\"  Min: {label_metadata['ret'].min():.6f}\")\n",
    "    print(f\"  Max: {label_metadata['ret'].max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc19f2",
   "metadata": {},
   "source": [
    "## 4. Compute Sample Weights\n",
    "\n",
    "**Sample weighting based on:**\n",
    "- **Concurrency**: How many labels overlap at each timestamp\n",
    "- **Return attribution**: Importance based on magnitude of returns\n",
    "- **Time decay**: Optional decay to emphasize recent observations\n",
    "\n",
    "**Note:** If concurrency weights aren't available, we'll use uniform weights and compute them in the model notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e86a7",
   "metadata": {},
   "source": [
    "## 5. Feature Normalization (MinMax Scaling)\n",
    "\n",
    "**Important:** We normalize features to [0, 1] range to:\n",
    "- Ensure all features contribute equally to the model\n",
    "- Improve convergence for tree-based models\n",
    "- Make feature importance more interpretable\n",
    "\n",
    "**Note:** We fit the scaler on training data and transform both train and test to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any extreme values or issues before normalization\n",
    "print(\"Feature statistics check:\")\n",
    "print(f\"  Features with inf: {np.isinf(X).sum().sum()}\")\n",
    "print(f\"  Features with NaN: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Check for constant columns (would cause issues in normalization)\n",
    "constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
    "if constant_cols:\n",
    "    print(f\"\\n⚠ WARNING - Constant columns detected ({len(constant_cols)}): {constant_cols}\")\n",
    "    print(f\"  These will be removed before normalization\")\n",
    "    X = X.drop(columns=constant_cols)\n",
    "    feature_cols = X.columns.tolist()\n",
    "else:\n",
    "    print(f\"  ✓ No constant columns detected\")\n",
    "\n",
    "print(f\"\\nFinal feature count: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e05352",
   "metadata": {},
   "source": [
    "## 6. Time-Based Train/Test Split\n",
    "\n",
    "**Critical for time-series:**\n",
    "- No shuffling (preserves temporal order)\n",
    "- Train on earlier data, test on later data\n",
    "- Simulates real-world deployment scenario\n",
    "- Standard split: 70% train, 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split (70/30)\n",
    "split_ratio = 0.70\n",
    "split_idx = int(len(data) * split_ratio)\n",
    "\n",
    "# Split data\n",
    "X_train = X.iloc[:split_idx].copy()\n",
    "X_test = X.iloc[split_idx:].copy()\n",
    "y_train = y.iloc[:split_idx].copy()\n",
    "y_test = y.iloc[split_idx:].copy()\n",
    "\n",
    "metadata_train = label_metadata.iloc[:split_idx].copy()\n",
    "metadata_test = label_metadata.iloc[split_idx:].copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN/TEST SPLIT (Time-Based, No Shuffling)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Split ratio: {split_ratio:.0%} train / {1-split_ratio:.0%} test\")\n",
    "print(f\"Split index: {split_idx:,}\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  Shape: {X_train.shape}\")\n",
    "print(f\"  Date range: {X_train.index[0]} to {X_train.index[-1]}\")\n",
    "print(f\"  Label distribution:\")\n",
    "for label, count in y_train.value_counts().sort_index().items():\n",
    "    pct = count / len(y_train) * 100\n",
    "    label_name = {-1: 'STOP_LOSS', 0: 'TIMEOUT', 1: 'PROFIT_TARGET'}.get(label, 'UNKNOWN')\n",
    "    print(f\"    {label_name:15s} ({label:2d}): {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Shape: {X_test.shape}\")\n",
    "print(f\"  Date range: {X_test.index[0]} to {X_test.index[-1]}\")\n",
    "print(f\"  Label distribution:\")\n",
    "for label, count in y_test.value_counts().sort_index().items():\n",
    "    pct = count / len(y_test) * 100\n",
    "    label_name = {-1: 'STOP_LOSS', 0: 'TIMEOUT', 1: 'PROFIT_TARGET'}.get(label, 'UNKNOWN')\n",
    "    print(f\"    {label_name:15s} ({label:2d}): {count:6,} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a87b41",
   "metadata": {},
   "source": [
    "## 7. Normalize Features with MinMax Scaler\n",
    "\n",
    "**Fit on train, transform both train and test** to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ba804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scaler on training data only\n",
    "print(\"Fitting MinMaxScaler on training data...\")\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both train and test\n",
    "print(\"Transforming features to [0, 1] range...\")\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train),\n",
    "    index=X_train.index,\n",
    "    columns=X_train.columns\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    index=X_test.index,\n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "print(f\"✓ Normalization complete\")\n",
    "print(f\"\\nScaled feature statistics (train):\")\n",
    "print(f\"  Min: {X_train_scaled.min().min():.6f}\")\n",
    "print(f\"  Max: {X_train_scaled.max().max():.6f}\")\n",
    "print(f\"  Mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"  Std: {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "print(f\"\\nScaled feature statistics (test):\")\n",
    "print(f\"  Min: {X_test_scaled.min().min():.6f}\")\n",
    "print(f\"  Max: {X_test_scaled.max().max():.6f}\")\n",
    "print(f\"  Mean: {X_test_scaled.mean().mean():.6f}\")\n",
    "print(f\"  Std: {X_test_scaled.std().mean():.6f}\")\n",
    "\n",
    "# Verify no NaN or inf after scaling\n",
    "print(f\"\\nData quality check (post-scaling):\")\n",
    "print(f\"  Train - NaN: {X_train_scaled.isnull().sum().sum()}, Inf: {np.isinf(X_train_scaled).sum().sum()}\")\n",
    "print(f\"  Test - NaN: {X_test_scaled.isnull().sum().sum()}, Inf: {np.isinf(X_test_scaled).sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af46cb",
   "metadata": {},
   "source": [
    "## 8. Save Processed Datasets\n",
    "\n",
    "Save **ready-to-use** datasets for model training:\n",
    "- Features (normalized)\n",
    "- Labels\n",
    "- Sample weights (uniform for now, can be updated later)\n",
    "- Label metadata (for analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('data/training')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING PROCESSED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save train set\n",
    "train_features_file = output_dir / f'X_train_triple_barrier_{timestamp}.csv'\n",
    "train_labels_file = output_dir / f'y_train_triple_barrier_{timestamp}.csv'\n",
    "train_weights_file = output_dir / f'weights_train_triple_barrier_{timestamp}.csv'\n",
    "train_metadata_file = output_dir / f'metadata_train_triple_barrier_{timestamp}.csv'\n",
    "\n",
    "X_train_scaled.to_csv(train_features_file)\n",
    "y_train.to_csv(train_labels_file, header=True)\n",
    "\n",
    "metadata_train.to_csv(train_metadata_file)\n",
    "\n",
    "print(f\"Train set saved:\")\n",
    "print(f\"  Features: {train_features_file.name}\")\n",
    "print(f\"  Labels: {train_labels_file.name}\")\n",
    "print(f\"  Weights: {train_weights_file.name}\")\n",
    "print(f\"  Metadata: {train_metadata_file.name}\")\n",
    "\n",
    "# Save test set\n",
    "test_features_file = output_dir / f'X_test_triple_barrier_{timestamp}.csv'\n",
    "test_labels_file = output_dir / f'y_test_triple_barrier_{timestamp}.csv'\n",
    "test_weights_file = output_dir / f'weights_test_triple_barrier_{timestamp}.csv'\n",
    "test_metadata_file = output_dir / f'metadata_test_triple_barrier_{timestamp}.csv'\n",
    "\n",
    "X_test_scaled.to_csv(test_features_file)\n",
    "y_test.to_csv(test_labels_file, header=True)\n",
    "weights_test.to_csv(test_weights_file, header=True)\n",
    "metadata_test.to_csv(test_metadata_file)\n",
    "\n",
    "print(f\"\\nTest set saved:\")\n",
    "print(f\"  Features: {test_features_file.name}\")\n",
    "print(f\"  Labels: {test_labels_file.name}\")\n",
    "print(f\"  Weights: {test_weights_file.name}\")\n",
    "print(f\"  Metadata: {test_metadata_file.name}\")\n",
    "\n",
    "# Save feature names for reference\n",
    "feature_names_file = output_dir / f'feature_names_triple_barrier_{timestamp}.txt'\n",
    "with open(feature_names_file, 'w') as f:\n",
    "    f.write(\"Triple-Barrier Feature Names (Bollinger Mean Reversion)\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    for i, col in enumerate(X_train_scaled.columns, 1):\n",
    "        f.write(f\"{i}. {col}\\n\")\n",
    "\n",
    "print(f\"\\nFeature names: {feature_names_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ALL DATASETS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDatasets ready for model training in: {output_dir}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Compute concurrency weights (optional) using concurrency_weights.ipynb\")\n",
    "print(f\"  2. Train Random Forest with weighted samples in models.ipynb\")\n",
    "print(f\"  3. Compare with trend-scanning approach\")\n",
    "print(f\"\\nNote: Currently using Bollinger mean reversion strategy\")\n",
    "print(f\"      Can change to MA crossover in meta_labeling.ipynb for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8fcd3",
   "metadata": {},
   "source": [
    "## 9. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f81a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTrain Set:\")\n",
    "print(f\"  Observations: {len(X_train_scaled):,}\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Date range: {X_train_scaled.index[0]} to {X_train_scaled.index[-1]}\")\n",
    "print(f\"  Label distribution:\")\n",
    "for label, count in y_train.value_counts().sort_index().items():\n",
    "    pct = count / len(y_train) * 100\n",
    "    label_name = {-1: 'STOP_LOSS', 0: 'TIMEOUT', 1: 'PROFIT_TARGET'}.get(label, 'UNKNOWN')\n",
    "    print(f\"    {label_name}: {count:,} ({pct:.2f}%)\")\n",
    "print(f\"  Sample weights: mean={weights_train.mean():.4f}, std={weights_train.std():.4f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Observations: {len(X_test_scaled):,}\")\n",
    "print(f\"  Features: {X_test_scaled.shape[1]}\")\n",
    "print(f\"  Date range: {X_test_scaled.index[0]} to {X_test_scaled.index[-1]}\")\n",
    "print(f\"  Label distribution:\")\n",
    "for label, count in y_test.value_counts().sort_index().items():\n",
    "    pct = count / len(y_test) * 100\n",
    "    label_name = {-1: 'STOP_LOSS', 0: 'TIMEOUT', 1: 'PROFIT_TARGET'}.get(label, 'UNKNOWN')\n",
    "    print(f\"    {label_name}: {count:,} ({pct:.2f}%)\")\n",
    "print(f\"  Sample weights: mean={weights_test.mean():.4f}, std={weights_test.std():.4f}\")\n",
    "\n",
    "print(f\"\\nFeature Normalization:\")\n",
    "print(f\"  Method: MinMaxScaler\")\n",
    "print(f\"  Range: [0, 1]\")\n",
    "print(f\"  Fitted on: Train set only\")\n",
    "print(f\"  Applied to: Both train and test\")\n",
    "\n",
    "print(f\"\\nPrimary Strategy:\")\n",
    "print(f\"  Strategy: Bollinger Band Mean Reversion\")\n",
    "print(f\"  Window: 20, Std: 2.0\")\n",
    "print(f\"  Entry filter: CUSUM filter on volatility\")\n",
    "print(f\"  Triple-barrier: pt_sl=[1, 2], vertical_barrier=50 bars\")\n",
    "\n",
    "print(f\"\\n✓ Data preparation complete!\")\n",
    "print(f\"✓ Ready for Random Forest training with triple-barrier labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preview of prepared data\n",
    "print(\"Sample of prepared training data:\")\n",
    "print(\"\\nFeatures (first 3 rows, first 10 columns):\")\n",
    "print(X_train_scaled.iloc[:3, :10])\n",
    "print(\"\\nLabels (first 10):\")\n",
    "print(y_train.head(10))\n",
    "print(\"\\nSample weights (first 10):\")\n",
    "print(weights_train.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
