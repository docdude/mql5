{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d06c604",
   "metadata": {},
   "source": [
    "# Model Training: Triple-Barrier (Meta-Labeling)\n",
    "\n",
    "This notebook trains a Random Forest classifier for meta-labeling with the Bollinger Band mean reversion strategy.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load preprocessed training data (features, labels, metadata)\n",
    "2. Train Random Forest classifier\n",
    "3. Evaluate on test set with confusion matrix and metrics\n",
    "4. Analyze feature importance\n",
    "5. Save trained model for deployment\n",
    "\n",
    "**Meta-Labeling Context:**\n",
    "- Binary classification: 0 (SKIP_TRADE) vs 1 (TAKE_TRADE)\n",
    "- Purpose: Filter false positives from primary Bollinger strategy\n",
    "- Features: 62 technical indicators on EURUSD tick bars\n",
    "- Labels: ~10k strategy signals (not every bar)\n",
    "\n",
    "**Model:** Random Forest with sample weights (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26dcf98",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Training Data\n",
    "\n",
    "Load the most recent preprocessed dataset from `training_data_triple_barrier.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22677985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent preprocessed files\n",
    "data_dir = Path('data/training')\n",
    "\n",
    "# Get all triple_barrier training files sorted by timestamp\n",
    "train_files = sorted(data_dir.glob('X_train_triple_barrier_*.csv'))\n",
    "\n",
    "if not train_files:\n",
    "    raise FileNotFoundError(\"No preprocessed training data found. Run training_data_triple_barrier.ipynb first.\")\n",
    "\n",
    "# Extract timestamp from most recent file\n",
    "latest_file = train_files[-1]\n",
    "timestamp = latest_file.stem.split('_')[-2] + '_' + latest_file.stem.split('_')[-1]\n",
    "\n",
    "print(f\"Loading preprocessed data from: {timestamp}\")\n",
    "\n",
    "# Load all files with matching timestamp\n",
    "X_train = pd.read_csv(data_dir / f'X_train_triple_barrier_{timestamp}.csv', index_col=0, parse_dates=True)\n",
    "y_train = pd.read_csv(data_dir / f'y_train_triple_barrier_{timestamp}.csv', index_col=0, parse_dates=True)\n",
    "X_test = pd.read_csv(data_dir / f'X_test_triple_barrier_{timestamp}.csv', index_col=0, parse_dates=True)\n",
    "y_test = pd.read_csv(data_dir / f'y_test_triple_barrier_{timestamp}.csv', index_col=0, parse_dates=True)\n",
    "metadata_train = pd.read_csv(data_dir / f'metadata_train_triple_barrier_{timestamp}.csv', index_col=0, parse_dates=True)\n",
    "metadata_test = pd.read_csv(data_dir / f'metadata_test_triple_barrier_{timestamp}.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"  Train samples: {len(X_train):,}\")\n",
    "print(f\"  Test samples: {len(X_test):,}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Train date range: {X_train.index[0]} to {X_train.index[-1]}\")\n",
    "print(f\"  Test date range: {X_test.index[0]} to {X_test.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6bca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "print(\"Label Distribution (Train):\")\n",
    "print(y_train['bin'].value_counts())\n",
    "print(f\"\\nClass balance: {y_train['bin'].value_counts(normalize=True)}\")\n",
    "\n",
    "print(\"\\nLabel Distribution (Test):\")\n",
    "print(y_test['bin'].value_counts())\n",
    "print(f\"\\nClass balance: {y_test['bin'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691f3e5",
   "metadata": {},
   "source": [
    "## 2. Train Random Forest Classifier\n",
    "\n",
    "Train a Random Forest with reasonable default parameters. We'll use:\n",
    "- `n_estimators=500` - Number of trees\n",
    "- `max_depth=10` - Prevent overfitting\n",
    "- `min_samples_split=20` - Require minimum samples to split\n",
    "- `random_state=42` - Reproducibility\n",
    "- `n_jobs=-1` - Use all CPU cores\n",
    "- `class_weight='balanced'` - Handle class imbalance if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    bootstrap=True,\n",
    "    oob_score=True,  # Out-of-bag score for validation\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest classifier...\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Starting training at: {datetime.now().strftime('%H:%M:%S')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981060f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "rf_clf.fit(X_train, y_train['bin'])\n",
    "\n",
    "print(f\"\\n✓ Training completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"  OOB Score: {rf_clf.oob_score_:.4f}\")\n",
    "print(f\"  Number of features: {rf_clf.n_features_in_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3055bcd",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "### 3.1 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_train_pred = rf_clf.predict(X_train)\n",
    "y_test_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Generate prediction probabilities\n",
    "y_train_proba = rf_clf.predict_proba(X_train)[:, 1]\n",
    "y_test_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✓ Predictions generated\")\n",
    "print(f\"  Train predictions: {len(y_train_pred):,}\")\n",
    "print(f\"  Test predictions: {len(y_test_pred):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83245a52",
   "metadata": {},
   "source": [
    "### 3.2 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for train set\n",
    "train_accuracy = accuracy_score(y_train['bin'], y_train_pred)\n",
    "train_precision = precision_score(y_train['bin'], y_train_pred)\n",
    "train_recall = recall_score(y_train['bin'], y_train_pred)\n",
    "train_f1 = f1_score(y_train['bin'], y_train_pred)\n",
    "train_auc = roc_auc_score(y_train['bin'], y_train_proba)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test['bin'], y_test_pred)\n",
    "test_precision = precision_score(y_test['bin'], y_test_pred)\n",
    "test_recall = recall_score(y_test['bin'], y_test_pred)\n",
    "test_f1 = f1_score(y_test['bin'], y_test_pred)\n",
    "test_auc = roc_auc_score(y_test['bin'], y_test_proba)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<20} {'Train':<15} {'Test':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Accuracy':<20} {train_accuracy:<15.4f} {test_accuracy:<15.4f} {train_accuracy - test_accuracy:<15.4f}\")\n",
    "print(f\"{'Precision':<20} {train_precision:<15.4f} {test_precision:<15.4f} {train_precision - test_precision:<15.4f}\")\n",
    "print(f\"{'Recall':<20} {train_recall:<15.4f} {test_recall:<15.4f} {train_recall - test_recall:<15.4f}\")\n",
    "print(f\"{'F1 Score':<20} {train_f1:<15.4f} {test_f1:<15.4f} {train_f1 - test_f1:<15.4f}\")\n",
    "print(f\"{'ROC AUC':<20} {train_auc:<15.4f} {test_auc:<15.4f} {train_auc - test_auc:<15.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Interpretation\n",
    "if train_accuracy - test_accuracy > 0.1:\n",
    "    print(\"\\n⚠ Warning: Large train-test gap suggests overfitting\")\n",
    "else:\n",
    "    print(\"\\n✓ Train-test performance gap is reasonable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    y_test['bin'], \n",
    "    y_test_pred,\n",
    "    target_names=['SKIP_TRADE (0)', 'TAKE_TRADE (1)'],\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db1430",
   "metadata": {},
   "source": [
    "### 3.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117aa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test['bin'], y_test_pred)\n",
    "\n",
    "# Create labeled confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create annotations\n",
    "group_names = ['True Neg\\n(Correct Skip)', 'False Pos\\n(Wrong Take)', \n",
    "               'False Neg\\n(Missed Trade)', 'True Pos\\n(Correct Take)']\n",
    "group_counts = [f'{value:,}' for value in cm.flatten()]\n",
    "group_percentages = [f'{value:.2%}' for value in cm.flatten() / np.sum(cm)]\n",
    "\n",
    "labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', \n",
    "            xticklabels=['SKIP_TRADE (0)', 'TAKE_TRADE (1)'],\n",
    "            yticklabels=['SKIP_TRADE (0)', 'TAKE_TRADE (1)'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "\n",
    "plt.title('Confusion Matrix - Meta-Labeling (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display key metrics from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (Correct Skip):  {tn:,} ({tn/np.sum(cm):.2%})\")\n",
    "print(f\"  False Positives (Wrong Take):    {fp:,} ({fp/np.sum(cm):.2%})\")\n",
    "print(f\"  False Negatives (Missed Trade):  {fn:,} ({fn/np.sum(cm):.2%})\")\n",
    "print(f\"  True Positives (Correct Take):   {tp:,} ({tp/np.sum(cm):.2%})\")\n",
    "\n",
    "print(f\"\\nInterpretation for Meta-Labeling:\")\n",
    "print(f\"  • False Positives: Model says TAKE but strategy loses (cost: trade execution)\")\n",
    "print(f\"  • False Negatives: Model says SKIP but strategy wins (cost: missed profit)\")\n",
    "print(f\"  • Goal: Minimize false positives to filter bad signals from primary strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2900955",
   "metadata": {},
   "source": [
    "### 3.4 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test['bin'], y_test_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, 'b-', label=f'Random Forest (AUC = {test_auc:.4f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5000)', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Meta-Labeling (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC AUC Score: {test_auc:.4f}\")\n",
    "if test_auc > 0.7:\n",
    "    print(\"✓ Good discrimination between classes\")\n",
    "elif test_auc > 0.6:\n",
    "    print(\"⚠ Moderate discrimination - consider feature engineering\")\n",
    "else:\n",
    "    print(\"⚠ Poor discrimination - model needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515a541",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ba904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top 20 features\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in feature_importance.head(20).iterrows():\n",
    "    print(f\"{row['feature']:<30} {row['importance']:.6f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_n = 20\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "plt.barh(range(top_n), top_features['importance'])\n",
    "plt.yticks(range(top_n), top_features['feature'])\n",
    "plt.xlabel('Importance Score (Mean Decrease in Impurity)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title(f'Top {top_n} Feature Importances - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb866c1b",
   "metadata": {},
   "source": [
    "## 5. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate timestamp for model filename\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = models_dir / f'rf_meta_labeling_{timestamp}.pkl'\n",
    "\n",
    "# Save model with metadata\n",
    "model_data = {\n",
    "    'model': rf_clf,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'feature_importance': feature_importance,\n",
    "    'train_date_range': (str(X_train.index[0]), str(X_train.index[-1])),\n",
    "    'test_date_range': (str(X_test.index[0]), str(X_test.index[-1])),\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'metrics': {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1,\n",
    "        'test_auc': test_auc,\n",
    "        'oob_score': rf_clf.oob_score_\n",
    "    },\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, model_filename)\n",
    "\n",
    "print(f\"\\n✓ Model saved successfully\")\n",
    "print(f\"  Location: {model_filename}\")\n",
    "print(f\"  Size: {model_filename.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"\\nTo load this model later:\")\n",
    "print(f\"  model_data = joblib.load('{model_filename}')\")\n",
    "print(f\"  rf_clf = model_data['model']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250e0d1",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Review the model training results and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel Type: Random Forest Classifier (Meta-Labeling)\")\n",
    "print(f\"Strategy: Bollinger Band Mean Reversion\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"  Test samples: {len(X_test):,}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Train period: {X_train.index[0].date()} to {X_train.index[-1].date()}\")\n",
    "print(f\"  Test period: {X_test.index[0].date()} to {X_test.index[-1].date()}\")\n",
    "print(f\"\\nModel Performance (Test Set):\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {test_auc:.4f}\")\n",
    "print(f\"  OOB Score: {rf_clf.oob_score_:.4f}\")\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {idx+1}. {row['feature']}: {row['importance']:.6f}\")\n",
    "print(f\"\\nModel saved: {model_filename.name}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Run backtesting to evaluate trading performance\")\n",
    "print(\"  2. Compare with baseline (primary strategy without ML filter)\")\n",
    "print(\"  3. Try different probability thresholds to optimize precision/recall\")\n",
    "print(\"  4. Consider hyperparameter tuning (GridSearchCV, RandomSearchCV)\")\n",
    "print(\"  5. Experiment with sample weighting (concurrency, return attribution)\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
