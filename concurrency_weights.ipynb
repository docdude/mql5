{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f27263",
   "metadata": {},
   "source": [
    "# Phase 1: Concurrency & Sample Weights\n",
    "\n",
    "This notebook implements the sample weighting methodology from **MQL5 Article 19850: Label Concurrency**.\n",
    "\n",
    "## Objectives:\n",
    "1. Compute concurrent events count (how many labels overlap at each timestamp)\n",
    "2. Calculate uniqueness weights (corrects for temporal overlap)\n",
    "3. Calculate return attribution weights (for comparison only)\n",
    "4. Calculate time decay weights (gives more weight to recent observations)\n",
    "5. Compare all weighting methods\n",
    "6. Save weights for model training\n",
    "\n",
    "## Key Insight from Article:\n",
    "**Uniqueness weighting** consistently improves model performance by ensuring each observation's influence during training is proportional to its unique information content. This addresses the violation of the IID assumption in financial time series.\n",
    "\n",
    "**Performance Note**: This notebook uses optimized versions (5-10x faster than standard implementations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928992e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "✓ Using optimized functions (5-10x performance improvement)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import optimized functions (5-10x faster than standard versions)\n",
    "from optimized_concurrent import (\n",
    "    get_num_conc_events_optimized,\n",
    "    get_av_uniqueness_from_triple_barrier_optimized\n",
    ")\n",
    "from optimized_attribution import (\n",
    "    get_weights_by_return_optimized,\n",
    "    get_weights_by_time_decay_optimized\n",
    ")\n",
    "from load_data import load_bars\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(\"✓ Using optimized functions (5-10x performance improvement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caabd1b",
   "metadata": {},
   "source": [
    "## Step 1: Load Labeled Events\n",
    "\n",
    "Load the labeled events from `sides.ipynb` that contain:\n",
    "- **t1**: End time of the label (when barrier was hit)\n",
    "- **bin**: Binary label (0=timeout/stop loss, 1=profit target)\n",
    "- **ret**: Return of the trade\n",
    "- **side**: Position side (1=long, -1=short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4bff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading tick bars from: EURUSD_tick_bars_20251101_170526.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EURUSD tick bars...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded 686,033 tick bars\n",
      "INFO:   Start: 2023-01-02 07:33:51.458001\n",
      "INFO:   End: 2025-10-31 22:58:59.181001\n",
      "INFO:   Columns: ['open', 'high', 'low', 'close', 'tick_volume']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 686,033 bars\n",
      "  Date range: 2023-01-02 07:33:51.458001 to 2025-10-31 22:58:59.181001\n",
      "  Columns: ['open', 'high', 'low', 'close', 'tick_volume']\n"
     ]
    }
   ],
   "source": [
    "# Load bar data\n",
    "SYMBOL = 'EURUSD'\n",
    "BAR_TYPE = 'tick'\n",
    "\n",
    "print(f\"Loading {SYMBOL} {BAR_TYPE} bars...\")\n",
    "df = load_bars(SYMBOL, BAR_TYPE)\n",
    "close = df['close']\n",
    "\n",
    "print(f\"✓ Loaded {len(df):,} bars\")\n",
    "print(f\"  Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "784a29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading events from data\\EURUSD_triple_barrier_events.csv...\n",
      "\n",
      "✓ Loaded 4,377 labeled events\n",
      "  Columns: ['t1', 'trgt', 'ret', 'bin', 'side']\n",
      "\n",
      "Label distribution:\n",
      "bin\n",
      "0    0.463103\n",
      "1    0.536897\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample events:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>trgt</th>\n",
       "      <th>ret</th>\n",
       "      <th>bin</th>\n",
       "      <th>side</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-03 10:04:56.812001</th>\n",
       "      <td>2023-01-03 10:16:13.477001</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>-0.002159</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 10:05:15.772001</th>\n",
       "      <td>2023-01-03 10:17:36.057001</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>-0.001538</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 10:13:48.968001</th>\n",
       "      <td>2023-01-03 10:27:00.805001</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-0.001228</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 10:18:52.115001</th>\n",
       "      <td>2023-01-03 10:35:15.660001</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 10:30:48.800001</th>\n",
       "      <td>2023-01-03 11:02:12.825001</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>-0.001784</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    t1      trgt       ret  \\\n",
       "time                                                                         \n",
       "2023-01-03 10:04:56.812001  2023-01-03 10:16:13.477001  0.001016 -0.002159   \n",
       "2023-01-03 10:05:15.772001  2023-01-03 10:17:36.057001  0.001141 -0.001538   \n",
       "2023-01-03 10:13:48.968001  2023-01-03 10:27:00.805001  0.001753 -0.001228   \n",
       "2023-01-03 10:18:52.115001  2023-01-03 10:35:15.660001  0.001979 -0.001229   \n",
       "2023-01-03 10:30:48.800001  2023-01-03 11:02:12.825001  0.001909 -0.001784   \n",
       "\n",
       "                            bin  side  \n",
       "time                                   \n",
       "2023-01-03 10:04:56.812001    0     1  \n",
       "2023-01-03 10:05:15.772001    0     1  \n",
       "2023-01-03 10:13:48.968001    0     1  \n",
       "2023-01-03 10:18:52.115001    0     1  \n",
       "2023-01-03 10:30:48.800001    0     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load labeled events\n",
    "# Option 1: Load from saved CSV (if you've saved from sides.ipynb)\n",
    "events_file = Path('data') / f'{SYMBOL}_triple_barrier_events.csv'\n",
    "\n",
    "if events_file.exists():\n",
    "    print(f\"Loading events from {events_file}...\")\n",
    "    events = pd.read_csv(events_file, index_col=0, parse_dates=True)\n",
    "else:\n",
    "    print(\"ERROR: Events file not found!\")\n",
    "    print(f\"Please save your labeled events from sides.ipynb to: {events_file}\")\n",
    "    print(\"\\nYou can add this to sides.ipynb:\")\n",
    "    print(\"  events.to_csv('data/EURUSD_labeled_events.csv')\")\n",
    "    raise FileNotFoundError(f\"Events file not found: {events_file}\")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(events):,} labeled events\")\n",
    "print(f\"  Columns: {list(events.columns)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(events['bin'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSample events:\")\n",
    "events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b8489",
   "metadata": {},
   "source": [
    "## Step 2: Compute Concurrent Events Count\n",
    "\n",
    "For each timestamp in our data, count how many labeled events are \"active\" (their label period overlaps with that timestamp).\n",
    "\n",
    "**Why this matters**: When 5 events overlap at a timestamp, each event receives 1/5 of the information content at that timestamp. This helps us quantify redundancy in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ba491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing concurrent events count...\n",
      "(Using Numba-optimized version - 5-10x faster)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_num_conc_events_optimized() got an unexpected keyword argument 'events'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      6\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m num_conc_events = \u001b[43mget_num_conc_events_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Computed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: get_num_conc_events_optimized() got an unexpected keyword argument 'events'"
     ]
    }
   ],
   "source": [
    "# Compute concurrent events using optimized function\n",
    "print(\"Computing concurrent events count...\")\n",
    "print(\"(Using Numba-optimized version - 5-10x faster)\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "num_conc_events = get_num_conc_events_optimized(\n",
    "    close=close,\n",
    "    label_endtime=events,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Computed in {elapsed:.2f} seconds\")\n",
    "print(f\"\\nConcurrency Statistics:\")\n",
    "print(f\"  Mean concurrency: {num_conc_events.mean():.2f}\")\n",
    "print(f\"  Max concurrency: {num_conc_events.max():.0f}\")\n",
    "print(f\"  Median concurrency: {num_conc_events.median():.2f}\")\n",
    "print(f\"  Min concurrency: {num_conc_events.min():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize concurrency over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Time series plot\n",
    "axes[0].plot(num_conc_events.index, num_conc_events.values, linewidth=0.5, alpha=0.7)\n",
    "axes[0].axhline(num_conc_events.mean(), color='r', linestyle='--', label=f'Mean: {num_conc_events.mean():.2f}')\n",
    "axes[0].set_title('Number of Concurrent Events Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Concurrent Events')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution histogram\n",
    "axes[1].hist(num_conc_events.values, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(num_conc_events.mean(), color='r', linestyle='--', label=f'Mean: {num_conc_events.mean():.2f}')\n",
    "axes[1].set_title('Distribution of Concurrent Events', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Concurrent Events')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/concurrent_events.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"On average, {num_conc_events.mean():.1f} labels are active at any given timestamp.\")\n",
    "print(f\"This means each observation shares information with ~{num_conc_events.mean():.0f} other labels.\")\n",
    "print(f\"Without weighting, this redundancy causes overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed2f44",
   "metadata": {},
   "source": [
    "## Step 3: Compute Uniqueness Weights\n",
    "\n",
    "Calculate the **average uniqueness** for each event:\n",
    "- For each bar during an event's lifespan, compute `1 / concurrency_at_that_bar`\n",
    "- Average these values across the event's lifespan\n",
    "- Result: Events with low overlap get weight ~1.0, highly overlapping events get weight ~0.2-0.4\n",
    "\n",
    "**Article Finding**: This method improved F1 score by 6.7% (Bollinger) and 10.2% (MA Crossover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13856860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate uniqueness weights using optimized function\n",
    "print(\"Computing uniqueness weights...\")\n",
    "print(\"(Using Numba-optimized version - 3-5x faster)\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "uniqueness_weights = get_av_uniqueness_from_triple_barrier_optimized(\n",
    "    triple_barrier_events=events,\n",
    "    close_series=close,\n",
    "    num_conc_events=num_conc_events,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Computed in {elapsed:.2f} seconds\")\n",
    "print(f\"\\nUniqueness Weight Statistics:\")\n",
    "print(uniqueness_weights['tW'].describe())\n",
    "\n",
    "# Store in events DataFrame\n",
    "events['uniqueness'] = uniqueness_weights['tW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a725865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uniqueness weights\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series\n",
    "axes[0, 0].plot(uniqueness_weights.index, uniqueness_weights['tW'].values, linewidth=0.5, alpha=0.7)\n",
    "axes[0, 0].axhline(uniqueness_weights['tW'].mean(), color='r', linestyle='--', \n",
    "                   label=f'Mean: {uniqueness_weights[\"tW\"].mean():.3f}')\n",
    "axes[0, 0].set_title('Uniqueness Weights Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Uniqueness Weight')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[0, 1].hist(uniqueness_weights['tW'].values, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(uniqueness_weights['tW'].mean(), color='r', linestyle='--', \n",
    "                   label=f'Mean: {uniqueness_weights[\"tW\"].mean():.3f}')\n",
    "axes[0, 1].set_title('Distribution of Uniqueness Weights', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Uniqueness Weight')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: Concurrency vs Uniqueness\n",
    "sample_indices = events.index.intersection(num_conc_events.index)\n",
    "conc_at_start = num_conc_events.loc[sample_indices]\n",
    "unique_vals = uniqueness_weights.loc[sample_indices, 'tW']\n",
    "\n",
    "axes[1, 0].scatter(conc_at_start, unique_vals, alpha=0.3, s=10)\n",
    "axes[1, 0].set_title('Uniqueness vs Concurrency at Event Start', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Concurrent Events')\n",
    "axes[1, 0].set_ylabel('Uniqueness Weight')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by bins\n",
    "bins = pd.cut(uniqueness_weights['tW'], bins=5)\n",
    "bin_data = [uniqueness_weights['tW'][bins == b].values for b in bins.cat.categories]\n",
    "axes[1, 1].boxplot(bin_data, labels=[f'{b.left:.2f}-{b.right:.2f}' for b in bins.cat.categories])\n",
    "axes[1, 1].set_title('Uniqueness Weight Distribution by Bins', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Weight Range')\n",
    "axes[1, 1].set_ylabel('Uniqueness Weight')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/uniqueness_weights.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"Average uniqueness: {uniqueness_weights['tW'].mean():.3f}\")\n",
    "print(f\"This will be used as max_samples in BaggingClassifier\")\n",
    "print(f\"\\nWeight interpretation:\")\n",
    "print(f\"  1.0 = Completely unique (no overlap)\")\n",
    "print(f\"  0.5 = Moderate overlap\")\n",
    "print(f\"  <0.3 = Heavy overlap (mostly redundant information)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aca8c3",
   "metadata": {},
   "source": [
    "## Step 4: Compute Return Attribution Weights\n",
    "\n",
    "Calculate weights based on absolute returns during each event's lifespan.\n",
    "\n",
    "**⚠️ WARNING FROM ARTICLE**: This method caused model collapse in meta-labeling experiments. It's included here for comparison purposes only. **Do NOT use for production models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cf0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate return attribution weights using optimized function\n",
    "print(\"Computing return attribution weights...\")\n",
    "print(\"⚠️  WARNING: Article shows this method can cause model collapse!\")\n",
    "print(\"   Use only for comparison, not production.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "return_weights = get_weights_by_return_optimized(\n",
    "    triple_barrier_events=events,\n",
    "    close_series=close,\n",
    "    num_conc_events=num_conc_events,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Computed in {elapsed:.2f} seconds\")\n",
    "print(f\"\\nReturn Attribution Weight Statistics:\")\n",
    "print(return_weights.describe())\n",
    "\n",
    "# Store in events DataFrame\n",
    "events['return_attr'] = return_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize return attribution weights\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution\n",
    "axes[0].hist(return_weights.values, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(return_weights.mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {return_weights.mean():.3f}')\n",
    "axes[0].set_title('Distribution of Return Attribution Weights', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Weight')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time series\n",
    "axes[1].plot(return_weights.index, return_weights.values, linewidth=0.5, alpha=0.7)\n",
    "axes[1].axhline(return_weights.mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {return_weights.mean():.3f}')\n",
    "axes[1].set_title('Return Attribution Weights Over Time', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Weight')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/return_attribution_weights.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e60a4",
   "metadata": {},
   "source": [
    "## Step 5: Compute Time Decay Weights\n",
    "\n",
    "Apply time decay to give more weight to recent observations.\n",
    "\n",
    "**Note**: Decay is based on cumulative uniqueness (not chronological time) to avoid reducing weights too fast when observations are redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3318db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time decay weights using optimized function\n",
    "print(\"Computing time decay weights...\")\n",
    "print(\"(Using exponential decay with last_weight=0.5)\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "time_decay_weights = get_weights_by_time_decay_optimized(\n",
    "    triple_barrier_events=events,\n",
    "    close_series=close,\n",
    "    last_weight=0.5,  # Most recent gets 1.0, oldest gets 0.5\n",
    "    linear=False,  # Use exponential decay\n",
    "    av_uniqueness=uniqueness_weights,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✓ Computed in {elapsed:.2f} seconds\")\n",
    "print(f\"\\nTime Decay Weight Statistics:\")\n",
    "print(time_decay_weights.describe())\n",
    "\n",
    "# Store in events DataFrame\n",
    "events['time_decay'] = time_decay_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4006007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time decay pattern\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time series showing decay\n",
    "axes[0].plot(time_decay_weights.index, time_decay_weights.values, linewidth=1, alpha=0.8)\n",
    "axes[0].set_title('Time Decay Weights (Exponential)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Weight')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[1].hist(time_decay_weights.values, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(time_decay_weights.mean(), color='r', linestyle='--', \n",
    "                label=f'Mean: {time_decay_weights.mean():.3f}')\n",
    "axes[1].set_title('Distribution of Time Decay Weights', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Weight')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/time_decay_weights.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"Most recent observations have weight ~{time_decay_weights.iloc[-100:].mean():.3f}\")\n",
    "print(f\"Oldest observations have weight ~{time_decay_weights.iloc[:100].mean():.3f}\")\n",
    "print(f\"Decay factor: {time_decay_weights.iloc[:100].mean() / time_decay_weights.iloc[-100:].mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a00ad",
   "metadata": {},
   "source": [
    "## Step 6: Compare All Weighting Methods\n",
    "\n",
    "Create combined weights and analyze correlations between different weighting schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined weights (uniqueness × time_decay)\n",
    "events['combined'] = events['uniqueness'] * events['time_decay']\n",
    "\n",
    "# Normalize combined weights so they sum to number of samples\n",
    "events['combined'] = events['combined'] * len(events) / events['combined'].sum()\n",
    "\n",
    "print(\"Weight Correlations:\")\n",
    "weight_cols = ['uniqueness', 'return_attr', 'time_decay', 'combined']\n",
    "correlation_matrix = events[weight_cols].corr()\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Between Weighting Methods', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/weight_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91517f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all weighting methods side-by-side\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "weight_methods = {\n",
    "    'uniqueness': 'Uniqueness\\n(RECOMMENDED)',\n",
    "    'return_attr': 'Return Attribution\\n(⚠️ Can cause collapse)',\n",
    "    'time_decay': 'Time Decay',\n",
    "    'combined': 'Combined\\n(Uniqueness × Time Decay)'\n",
    "}\n",
    "\n",
    "for idx, (col, title) in enumerate(weight_methods.items()):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(events[col].values, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(events[col].mean(), color='r', linestyle='--', linewidth=2,\n",
    "               label=f'Mean: {events[col].mean():.3f}')\n",
    "    ax.axvline(events[col].median(), color='g', linestyle='--', linewidth=2,\n",
    "               label=f'Median: {events[col].median():.3f}')\n",
    "    \n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Weight')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/all_weights_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS FOR ALL WEIGHTING METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_stats = events[weight_cols].describe().T\n",
    "summary_stats['range'] = summary_stats['max'] - summary_stats['min']\n",
    "summary_stats['cv'] = summary_stats['std'] / summary_stats['mean']  # Coefficient of variation\n",
    "\n",
    "print(summary_stats.round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS FROM ARTICLE 19850:\")\n",
    "print(\"=\"*80)\n",
    "print(\"✓ RECOMMENDED: Use 'uniqueness' or 'combined' for model training\")\n",
    "print(\"✓ Set max_samples in BaggingClassifier to:\", f\"{events['uniqueness'].mean():.3f}\")\n",
    "print(\"✗ AVOID: 'return_attr' caused model collapse in article experiments\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0f4d3",
   "metadata": {},
   "source": [
    "## Step 7: Save Weights for Model Training\n",
    "\n",
    "Save all computed weights to CSV for use in Phase 2 (models.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "# Save weights\n",
    "weights_file = Path('data') / f'{SYMBOL}_sample_weights.csv'\n",
    "events[weight_cols].to_csv(weights_file)\n",
    "\n",
    "print(f\"✓ Saved sample weights to: {weights_file}\")\n",
    "print(f\"  Shape: {events[weight_cols].shape}\")\n",
    "print(f\"  Columns: {weight_cols}\")\n",
    "\n",
    "# Also save events with weights for convenience\n",
    "events_with_weights_file = Path('data') / f'{SYMBOL}_events_with_weights.csv'\n",
    "events.to_csv(events_with_weights_file)\n",
    "print(f\"\\n✓ Saved events with weights to: {events_with_weights_file}\")\n",
    "\n",
    "# Save concurrent events for future reference\n",
    "conc_file = Path('data') / f'{SYMBOL}_concurrent_events.csv'\n",
    "num_conc_events.to_csv(conc_file)\n",
    "print(f\"✓ Saved concurrent events to: {conc_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f0d539",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ✓ Computed concurrent events count (quantified label overlap)\n",
    "2. ✓ Calculated uniqueness weights (corrects for temporal redundancy)\n",
    "3. ✓ Calculated return attribution weights (for comparison)\n",
    "4. ✓ Calculated time decay weights (emphasizes recent data)\n",
    "5. ✓ Created combined weights (uniqueness × time_decay)\n",
    "6. ✓ Saved all weights for model training\n",
    "\n",
    "### Key Findings:\n",
    "- Average concurrency: Shows typical label overlap\n",
    "- Average uniqueness: Will be used as `max_samples` parameter\n",
    "- Weight correlations: Understand relationships between methods\n",
    "\n",
    "### Next Phase:\n",
    "**Phase 2: Model Training** (models.ipynb)\n",
    "- Implement PurgedKFold cross-validation\n",
    "- Train Random Forest with sample weights\n",
    "- Use BaggingClassifier with constrained samples\n",
    "- Compare weighted vs unweighted models\n",
    "\n",
    "### Article Citation:\n",
    "Based on: **Machine Learning Blueprint (Part 4): The Hidden Flaw in Your Financial ML Pipeline — Label Concurrency**  \n",
    "https://www.mql5.com/en/articles/19850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f08e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1 COMPLETE: CONCURRENCY & SAMPLE WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset: {SYMBOL} {BAR_TYPE} bars\")\n",
    "print(f\"Total events: {len(events):,}\")\n",
    "print(f\"Date range: {events.index[0]} to {events.index[-1]}\")\n",
    "print(f\"\\nConcurrency:\")\n",
    "print(f\"  Mean: {num_conc_events.mean():.2f}\")\n",
    "print(f\"  Max: {num_conc_events.max():.0f}\")\n",
    "print(f\"\\nRecommended Parameters for Phase 2:\")\n",
    "print(f\"  max_samples (BaggingClassifier): {events['uniqueness'].mean():.3f}\")\n",
    "print(f\"  sample_weight: Use 'uniqueness' or 'combined' column\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  ✓ {weights_file}\")\n",
    "print(f\"  ✓ {events_with_weights_file}\")\n",
    "print(f\"  ✓ {conc_file}\")\n",
    "print(f\"\\nReady for Phase 2: Model Training!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
